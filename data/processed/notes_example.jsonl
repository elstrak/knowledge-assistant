{"id": "вышка '24-25/нереляционные БД/1. 18 января.md", "title": "Нереляционные БД", "tags": [], "links": ["Screenshot 2025-01-18 at 11.20.12.png", "Screenshot 2025-01-18 at 11.22.20.png", "Screenshot 2025-01-18 at 11.35.02.png", "Screenshot 2025-01-18 at 11.49.21.png", "Screenshot 2025-01-18 at 11.53.31.png", "Screenshot 2025-01-18 at 11.57.37.png", "Screenshot 2025-01-18 at 12.00.45.png", "Screenshot 2025-01-18 at 12.01.39.png", "Screenshot 2025-01-18 at 12.03.55.png", "Screenshot 2025-01-18 at 12.12.20.png", "Screenshot 2025-01-18 at 12.22.06.png", "Screenshot 2025-01-18 at 12.53.11.png", "Screenshot 2025-01-18 at 12.54.29.png", "Screenshot 2025-01-18 at 12.57.43.png", "Screenshot 2025-01-18 at 12.57.52.png", "Screenshot 2025-01-18 at 13.03.06.png", "Screenshot 2025-01-18 at 13.05.05.png", "Screenshot 2025-01-18 at 13.05.36.png", "Screenshot 2025-01-18 at 13.09.56.png", "Screenshot 2025-01-18 at 13.10.19.png", "Screenshot 2025-01-18 at 13.12.54.png", "Screenshot 2025-01-18 at 13.18.50.png", "Screenshot 2025-01-18 at 13.24.57.png", "Screenshot 2025-01-18 at 13.25.37.png", "Screenshot 2025-01-18 at 13.26.28.png", "Screenshot 2025-01-18 at 13.28.34.png", "Screenshot 2025-01-18 at 13.29.44.png", "Screenshot 2025-01-18 at 13.30.16.png", "Screenshot 2025-01-18 at 13.31.06.png"], "content": "реляционные БД - набор табличек, в которых есть строки, эти строки обозначают один экземпляр сущности, а столбцы - характеристики, к-ю эту сущность описывают\n\n![[Screenshot 2025-01-18 at 11.20.12.png]]\nотношение = таблица \nатрибит или поле, в атрибуте содержится данные про одну сущность, кортеж\n\n![[Screenshot 2025-01-18 at 11.22.20.png]]добавляют в реляционные бд что-то новое постоянно, например, в mySql добавили тип данных вектор\nпри этом мы хотели бы хранить и обрабатывать много чего\n\nПРОБЛЕМЫ:\n1. нет оптимальных механизмов хранения (специальные типы данных есть не везде\n2. нет оптимальных механизмов взаимодействия (графы в реляционных бд)\n3. иногда много нулей и они занимают место + не несут ценности. решения:\n- спарс-матрица, к-я хранит координаты заполненных значений\n- убрать это поле (с нулем) вообще \n4. много данных, все тормозит\n\n**масштабирование** - процесс по увеличению производительности системы. масштабировать можно не только бд, это касается всех вычислительных систем\n\nмасштабирование бывает 2 видов:\n![[Screenshot 2025-01-18 at 11.35.02.png]]\n**вертикальное масштабирование:**\nувеличиваем 1 машину, мощнее процессоры, больше памяти. минусы: \n- в какой-то момент расширять это станет дорого и трудно.\n- сложнее вкладываться, сервера и прошивки серверов могут устаревать.\n\n**горизонтальное масштабирование:** - увеличиваем на сам комп, а их количество\nразделяем всё на части или по нескольким машинам:\n1. **партиционирование**- разделение таблиц по нескольким частям. например, по какому-то значению или группе значений\n\tплюсы:\n\t- параллельно выполняющиеся запросы\n\t- быстрый доступ к данным в определенном диапазоне\n\tминусы:\n\t- мы просто разделили данные на одной машине\n\tвизуально вот так\n\n![[Screenshot 2025-01-18 at 11.49.21.png]]\n2. **репликация** - создание копий баз данных на разных серверах\nплюсы:\n- можно разместить удаленно\n- балансировка нагрузки\n- безопасность при сгорании одного из серверов\nминусы:\n- храним полную копию\n- при росте числа узлов усложняется схема передачи![[Screenshot 2025-01-18 at 11.53.31.png]]\nmaster - основная БД. 2 реплики-бэкапа\n3. **шардирование/шардинг** - разделение данных по разным серверам. из минусов репликации - храним полную копию таблицы, шардинг - партиционирование рпеликации (делим и храним)\nплюсы:\n- храним меньше данных в одном месте\nминусы: \n- трудности со взаимодействием между шардами\n- мало какие реляционные базы данных поддерживают данныц тип\n![[Screenshot 2025-01-18 at 11.57.37.png]]\nреляционная модель плохо подходит для масштабирования:\n- проблемы со взаимодействием между данными\n- проблемы со скоростью обработки\n- проблемы с хранением и потерей данных\n\n\n![[Screenshot 2025-01-18 at 12.00.45.png]]\n\n| вертикальное                            | горизонтальное                                 |\n| --------------------------------------- | ---------------------------------------------- |\n| расширяем 1 машину                      | несколько машин                                |\n| ставим что-то дополнительное и улучшаем | можно скопировать БД, можно разбить на кусочки |\n|                                         |                                                |\n|                                         |                                                |\n![[Screenshot 2025-01-18 at 12.01.39.png]]\n\nитого: хотим хранить любой тип данных адекватно и при этом не иметь кучи пустых значений\n\n### Нереляционные БД\nНереляционные БД - другая модель хранения данных\nона завязана на некоторых принципах построения и управления базами, но четких критериев нет. это плюс (гибкая модель данных) и минус (очень сильно отходим от четкого представления о том, что есть что)\n\nтипы: ![[Screenshot 2025-01-18 at 12.03.55.png]]\nнереляционное хранение внедрили ещё в 1960х, но первой NoSQL СУБД считается Strozzi NoSQL. Карло Строцци переписал СУБД RDB авторства RAND под руководством Уолтера Хоббса. Программа вышла в 1998 году\n\nRBD была системой управления реляционными базами данных, использующая комплитяор SQL. Строцци переписал программу, чтобы к БД можно было обращаться с помощью shell-скриптов**\n\nФайл с программой Строцци подписал NoSQL (ого класс) !\n\n--------------\n*shell \n*Shell - интерфейс операционной системы. \n\n*Shell scripting означает написание скриптов ака файлов с кодом, в котором содержатся программы. такие скрипты обычно пишутся для автоматизации работы с Unix/Linux\nоболочки бывают разные, языки скриптования разные, ОС разные\n\n*Bash - одна из самых популярных оболочек. Такую можно использовать для управления СУБД через командную строку, если указать bash при запуске контейнера\n\n``sudo docker exec -it mysql-docker-container bash\n\n*Скрипты полезны (этот пример баш-скрипта к-й добавляет юзера в базу данных)\n```\n#!/bin/bash\nset -e\n\npsql -v ON_ERROR_STOP=1 --username\n\"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-ESQL\n\tCREATE USER docker ;\n\tCREATE DATABASE docker ;\n\tGRANT ALL PRIVELEGES ON DATABASE docker TO docker ;\nESQL\n\t\n```\n\n--------------\n\n![[Screenshot 2025-01-18 at 12.12.20.png]]\nNoSQL начала расширяться в 2010-е\nNoSQL - гибкая производительная технология, позволяющая содержать как маленькие, так и большие БД. есть минусы в виде того, что возможно вам понадобится несколько серверов\n\nПодвох: \n\n![[Screenshot 2025-01-18 at 12.22.06.png]]\n\nсвязь может теряться между шардами. соответственно какие-то части данныз могут не быть полными в определенный момент. нам бы хотелось чтобы даже при потере связи между серверами/шардами, они продолжали бы работать\nсогласно Брюеру, мы можем выбрать только 2.\nнапример, в целом все согласовано и доступно, но может теряться связь между узлами\n\nсуществует 3 типа систем:\n- CP - consistent + partition tolerant\n- CA - consistent + always available\n- AP - always available + partition tolerant\n\nтеорема САР немного оправдывает системы, разработчики к-х утверждают, что нельзя одновременно иметь согласованность и доступность, и во имя доступности мы чаще всего будем жертвовать согласованностью\n\nно мы не может не поддерживать согласованность совсем. можно немножко смягчить требования к модели -> нереляционные системы отвечают свойствам BASE:\n- Basic availability - система в общем и целом доступна, даже если что-то сломалось или упало соединение\n- soft-state - данные могут быть несогласованы в какие-то моменты времени (для системы с несколткими частями это легче)\n- eventually consistent - данные в итоге будут согласованы, даже если не сразу и не в один момент времени (например, одна реплика будет согласована с другой, но через час)\n\nCAP - всё ещё эвристическое утверждение\n\nпроектирование в нереляционных БД - сложная тема по нескольким причинам:\n1. зависит от типа БД в принципе: ключ-значение, документирование, графовые, колоночные\n2. нереляционные БД строятся на основе принципов САР и BASE. выбор СУБД и архитектура БД будут зависеть от того, что мы хотим получить от системы\n![[Screenshot 2025-01-18 at 12.53.11.png]]\n![[Screenshot 2025-01-18 at 12.54.29.png]]\n\nс инжереной точки зрения доступ и взаимодействие с БД происходит по API\n\n| SQL                                      | NoSQL                                           |\n| ---------------------------------------- | ----------------------------------------------- |\n| powerful declarative language construct  | the DB model is not relational                  |\n| schemas and metadata                     | focus on distributed and horizontal scalability |\n| consistency assistance                   | weak or no schema restrictions                  |\n| referential integrity and triggers       | data replication is easy                        |\n| recovery and logging                     | easy access is provided via an API              |\n| multi-user operation and synchronization | consistency model is not ACID                   |\n| users, roles and security                |                                                 |\n| indexing                                 |                                                 |\n|                                          |                                                 |\nNoSQL хорошо масштабируется горизонтально:\n1. многие виды позволяют логически разделить данные безболезненно  (например, коллекции в документоориентированной базе)\n2. отказались от ACID\n![[Screenshot 2025-01-18 at 12.57.43.png]]\n![[Screenshot 2025-01-18 at 12.57.52.png]]\n### Виды нереляционных баз данных\n\n1. Ключ-значение\nреференс - словарь\n```\nbook1: {\n'id': '0',\n'book': 'huy'\n}\n```\nRedis (remote dictionary service aka Redis) - нереляционная субд для хранения инфы в виде \"ключ-значение\"![[Screenshot 2025-01-18 at 13.03.06.png]]\nявляется системой управление *резидентными базами данных*, то есть размещаемыми в оперативной памяти\n\n2. документоориентированные\nреференс - json\n```\n{                   {\n'id': '0',           'id': '1',\n'book': 'huy'        'book': 'pizda'\n}                    }\n```\n\nMongoDB - опенсорсная СУБД. не требует описания схемы, самая используемая документоориентированная бд\n![[Screenshot 2025-01-18 at 13.05.05.png]]\n![[Screenshot 2025-01-18 at 13.05.36.png]]\n\nSchemaless - отсутствие схемы или ее гибкость. \nнапример, у нас есть 2 дока, каждый из к-х описывает книгу. у книги может не быть автора, но есть редактор. может быть неизвестен год публикации или издатель\nв реляционной бд на месте пропусков будут нули - и они тоже занимают место. в документоориентированной бд пустые ключи можно просто пропустить. при этом добавили возможность создавать схемы в mongoDB (может быть полезно)\n![[Screenshot 2025-01-18 at 13.09.56.png]]\n\n3. Графовые\nреференс: графы\n![[Screenshot 2025-01-18 at 13.10.19.png]]\nNeo4j - самая популярная графовая СУБД. в качестве языка запросов - Cypher\n\n4. колоночные\n![[Screenshot 2025-01-18 at 13.12.54.png]]\nв реляционных бд мы все хранили по строкам, в колоночных - по колонках\nколоночное хранение позволяет в огромных широких таблицах выбирать только те колонки, которые мы хотим\n\nClickHouse - разработка Яндекс. в качестве языка запросов - обычный SQL\n![[Screenshot 2025-01-18 at 13.18.50.png]]![[Screenshot 2025-01-18 at 13.24.57.png]]\n![[Screenshot 2025-01-18 at 13.25.37.png]]\n![[Screenshot 2025-01-18 at 13.26.28.png]]\n![[Screenshot 2025-01-18 at 13.28.34.png]]\n\n![[Screenshot 2025-01-18 at 13.29.44.png]]\n![[Screenshot 2025-01-18 at 13.30.16.png]]\n![[Screenshot 2025-01-18 at 13.31.06.png]]\n\n\n\n\n", "created": "2025-01-18T13:31:14", "modified": "2025-01-18T13:31:14"}
{"id": "CS courses, hse/алгоритмы и структуры данных/0. основные алгоритмы и структуры данных.md", "title": "0. основные алгоритмы и структуры данных", "tags": ["сортировка_подсчетом", "сортировка_пузырьком", "сортировка_выбором", "сортировка_вставками", "radix_sort", "merge_sort"], "links": [], "content": "\n| алгоритм                       | асимптотическая сложность | краткое описание      |\n| ------------------------------ | ------------------------- | --------------------- |\n| пузырьковая сортировка         | $O(N^2)$                  | #сортировка_пузырьком |\n| сортировка выбором             | $O(N^2)$                  | #сортировка_выбором   |\n| сортировка вставками           | $O(N^2)$                  | #сортировка_вставками |\n| сортировка подсчетом           | $O(N)$                    | #сортировка_подсчетом |\n| сортировка слиянием            | $O(N\\log N)$              | #merge_sort           |\n| быстрая сортировка             | $O(N\\log N)$              |                       |\n| цифровая сортировка            | $O(N K)$                  | #radix_sort           |\n| линейный поиск                 |                           |                       |\n| бинарный поиск                 |                           |                       |\n| поиск верхней и нижней границы |                           |                       |\n| троичный поиск                 |                           |                       |\n| статистический массив          |                           |                       |\n| связный список                 |                           |                       |\n| саморасширяющийся массив       |                           |                       |\n| стек                           |                           |                       |\n| очередь                        |                           |                       |\n| дек                            |                           |                       |\n| приоритетная очередь           |                           |                       |\n| куча                           |                           |                       |\n| сортировка кучей               |                           |                       |\n\n1. Понятие асимптотической сложности. В чем плюсы и минусы асимптотической оценки сложности\n    \n2. Виды сортировок. Квадратичные сортировки. Сортировка подсчетом. Цифровая сортировка\n    \n3. Сортировка слиянием и быстрая сортировка.\n    \n4. Сортировка кучей\n    \n5. Алгоритмы поиска. Бинарный поиск. Правая и левая границы\n    \n6. Устройство массива. Как работает статический и динамический массив, асимптотика операций.\n    \n7. Связный список. Типы связных списков. Чем отличается от массива, как устроен внутри, асимптотика.\n    \n8. Как работает стек, его асимптотика. Как реализовать стек через массив или список\n    \n9. Как работает очередь, ее асимптотика. Как реализовать очередь через массив или список\n    \n10. Как работает приоритетная очередь, ее асимптотика. Реализация приоритетной очереди.\n    \n11. Понятия хеш-функции и хеш-таблицы. Коллизия. Способы разрешения коллизий.\n    \n12. Динамическое программирование. Как решать задачи ДП. Задачи ДП про кузнечика и черепашку.\n    \n13. Задача о рюкзаке. Расстояние Левенштейна.\n    \n14. Наибольшая общая подпоследовательность. Наибольшая возрастающая подпоследовательность\n    \n15. Хеширование строк. Полиномиальный хеш. Способы применения. \n    \n16. Префикс функция. Алгоритм Кнута-Морриса-Пратта.\n    \n17. Графы. Способы хранения графа. Обход графа в глубину и в ширину.\n    \n18. Поиск компонент связности и циклов в графе. Топологическая сортировка.\n    \n19. Поиск кратчайших путей в графе. Алгоритм Дейкстры\n    \n20. Алгоритм Форда-Беллмана. Поиск цикла отрицательного веса\n  ", "created": "2025-06-16T22:42:37", "modified": "2025-06-16T22:42:23"}
{"id": "собесы/2. линейные модели.md", "title": "Линейная регрессия и МНК", "tags": ["softmax", "регуляризация", "линейная_классификация", "многоклассовая_классификация", "линейная_регрессия", "метод_наименьших_квадратов", "сигмоида", "стохастический_гд", "MSE", "градиент", "логистическая_регрессия"], "links": ["Screenshot 2025-08-14 at 14.36.53.png", "Screenshot 2025-08-14 at 14.39.46.png", "Screenshot 2025-08-14 at 16.02.32.png"], "content": "Математически задачи можно описать как поиск отображения из множества Х в множество таргетов:\n- **классификация**: $X→\\{0,1,…,K\\}$, где $0,…,K$ – номера классов\n- **регрессия**: $X→R$.\n\nнам нужно найти такое отображение, которое лучше всего приближает истинное значение из множества таргетов. \nдля линейных моделей мы ищем это отображение в определенном параметризированном семействе функций - в линейных функциях вида\n$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + w_0$$\n$(x_1,x_2..., x_D)$ - вектор признаков, соответствующий объекту выборки\n$(w_1,w_2..., w_D, w_0)$ - вектор параметров модели, весов\nна предсказание можно смотреть как на взвешенную сумма признаков объекта\n$w_0$ - свободный коэффициент, или сдвиг (bias)\n\nлинейные модели хороши тем, что просты и легко интерпретируемы\n\n## Линейная регрессия и МНК\n#метод_наименьших_квадратов\n#линейная_регрессия \n\nмоделируем задачу как функцию вида $f_w(x_i) = <w, x_i>$\nмы хотим, чтобы наша функция как можно лучше приближала зависимость на парах $(x_i, y_i)$ = измеряем качество модели и минимизируем ее ошибку, меняя обучаемые параметры\nфункция, оценивающая, как часто модель ошибается - **функция потерь**. она должна быть легко оптимизируема\n\nвозьмем квадрат $L^2$ нормы вектора разницы предсказаний модели и истинного таргета y:\n$$L(f,X,y) = \\sum^N_{i=1} (y_i - <x_i,w>)^2$$\nне очень хорош, для выборок разного размера. лучше смотреть на среднеквадратичное отклонение (MSE)\n#MSE \n$$L(f,X,y) = \\frac{1}{N} \\sum^N_{i=1} (y_i - <x_i,w>)^2$$\n**функционалом** называют функции, которые получают на вход функции, а на выходе выдают число, оценивающее работу этой функции. чем меньше это число, тем точнее инпут функция, то есть нам надо минимизировать функционал по w:\n$$||y - Xw||^2_2 \\rightarrow \\min_w$$\nдва решения:\n1) точное аналитическое\nзадача звучит как \"найти линейную комбинацию столбцов $x^1,…,x^D$, которая наилучшим способом приближает столбец y по евклидовой норме – то есть найти **проекцию** вектора y на подпространство, образованное векторами $x^1,…,x^D$\"\nвычислительная сложность аналитического решения - $O(D^2N+D^3)$, где $N$ - длина выборки, $D$ - число признаков у одного объекта\nпроблема точного решения:\n- вычислительное обращение больших матриц затратно\n\n2) приближенный числовой метод\n#градиент\nГрадиент функции - это вектор, который указывает направление наискорейшего возрастания функции в данной точке, а антиградиент - в сторону ее наискорейшего убывания\n\nимея некоторое приближение параметра w, мы можем его улучшить, посчитав градиент функции потерь в точке и сдвинув вектор весов в направлении антиградиента:\n$$w_j \\rightarrow w_j - \\alpha \\frac{d}{dw_j}L(f_w,X,y)$$\n$\\alpha$ - **темп обучения**, контролирует величину шага в направлении антиградиента\nградиентный спуск для функции потерь MSE:\n$$\\nabla_wL = \\frac{2}{N}X^T(Xw - y)$$\nалгоритм градиентного спуска:\n```\nw = random_normal() # инициализация\nrepeat S times:\n\tf = X.dot(w)\n\terr = f - y\n\tgrad = 2 * X.T.dot(err) / N\n\tw -= alpha * grad\n```\n\nвычислительная сложность градиентного спуска - $O(NDS)$, где N - длина выборки, D - число признаков у одного объекта, S - количество итераций\n\n### стохастический градиентный спуск\n#стохастический_гд \nна каждом шаге ГД мы вычисляем градиент по всей выборке, что может быть потенциально дорогим, поэтому мы заменяем градиент по всей выборке оценкой на на подвыборке = на батче\n\nесли функция потерь имеет вид суммы по отдельным парам объект таргет:\n$$\nL(w,X,y) = \\frac{1}{N} \\sum^N_{i=1} L (w,x_i, y_i)$$\nа градиент выглядит так:\n$$\\nabla_w L(w,X,y) = \\frac{1}{N} \\sum^N_{i=1}\\nabla_wL(w,x_i,y_i)$$\nто мы берем оценку\n$$\\nabla_wL(w,X,y) \\sim \\frac{1}{B} \\sum^B_{t=1}\\nabla_wL(w,x_{i_t},y_{i_t})$$\nдля некотрого подмножества пар (батч)\nвводим новые гиперпараметры B(размер батча) и E (количество эпох)\n\nалгоритм: \n```\nw = normal(0,1)\nrepeat E times:\n\tfor i = B, i <=n, i+B\n\t\tX_batch = X[i-B : i]\n\t\ty_batch = y[i-B : i]\n\t\tf = X_batch.dot(w) # посчитать предсказание\n\t\terr = f - y_batch # посчитать ошибку\n\t\tgrad = 2 * X_batch.T.dot(err) / B # посчитать градиент\n\t\tw -= alpha * grad\n```\nсложность - $O(NDE)$\nсделали в $N/B$ раз больше шагов, то есть веса претерпели больше обновлений, но сложность такая же, как у обычного ГД\ncложность по памяти можно довести до $O(BD)$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом).\nпри этом лучше брать побольше размер батча, потому что чтение с диска - более затратная по времени операция, чем чтение из оперативной памяти\n\n### регуляризация *повторить\n#регуляризация \nесли два признака линейно зависимы, ранг матрицы будет D => будет более чем одно решение, и оно может быть сколь угодно большим по модулю => вычислительная трудность\n\nтакое редко, но часто признаки могут быть приближенно линейно зависимыми - это ситуация **мультиколлинеарности**\nпроблемы, описанные выше, будут возникать и в этой ситуации\n\nдля решения этой проблемы, задачу регуляризуют - добавляют дополнительные ограничения на вектор весов\nэто ограничение - $L^1$ или $L^2$ норма\nзадача будет выглядеть следующим образом:\n$$\\min_wL(f,X,y) = \\min_w(||Xw-y||^2_2 + \\lambda||w||^k_k)$$\nгде $||w||^k_k$ может быть\n$$||w||^2_2 = w^2_1 + ... + w^2_D$$\nлибо\n$$||w||_1^1 = |w_1| + ... + |w_D| $$\nэта добавка называется регуляризационным членом или регуляризатором, а лямбда - коэффициентом регуляризации\n\nкоэффициент лямбла подбирают по логарифмической шкале и он сильно влияет на качество модели. вес $w_0$ соответствует отступу от начала координат (признаку из всех единичек), и он не регуляризуется\n\nградиент функции потерь после этого выглядит так:\n$L(f_w,X,y) = ||X_w - y||^2 + \\lambda||w||^2$\n$\\nabla_wL(f_w,X,y) = 2X^T(Xw - y) + 2\\lambda w$\n\nрегуляризацию можно определеять для любой функции потерь, в том числе и для задачи классификации\n\nодна из полезных особенностей $L^1$ регуляризации - ее применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результат регуляризация обнуляется = удаляем признаки, слабо влияющие на таргет. \n\n###  другие функции потерь\n\n**MSE**\n$MSE(y, \\hat{y}) = \\frac{1}{N} \\sum^N_{i=1} (y_i - \\hat{y}_i)^2$\n\n**MAE**\nmean absolute error/абсолютная ошибка\n\n$MAE(y, \\hat{y}) = \\frac{1}{N} \\sum^N_{i=1} |y_i - \\hat{y}_i|$\n\n- существенно меньший вклад в ошибку будут вносить примеры, сильно удаленные от ответов модели\n\n**MAPE**\nmean absolute percentage error/относительная ошибка\n\n$MAPE(y, \\hat{y}) = \\frac{1}{N} \\sum^N_{i=1} |\\frac{ \\hat{y}_i - y_i}{y_i}|$\n- используется в задачах прогнозирования, когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок и не хотим штрафовать модель за предсказание 2к вместо 1к сильнее, чем за предсказание 2 вместо 1\n\n##  Линейная классификация\n#линейная_классификация\nдля бинарной классификация цель обучить линейную модель так, чтобы плоскость, которую она задает, как можно лучше отделяла объекты одного класса от другого\n\nитоговое предсказание вычисляется по формуле $y = sign<w,x_i>$\n\nмы хотим минимизировать число ошибок классификатора, то есть\n$$\\sum_i I[y_i \\langle w,x_i\\rangle < 0] \\rightarrow \\min_w$$\nвеличина $M = y_i\\langle w,x_i \\rangle$ называется **отступом (margin)** классификатора. такая функция называется **misclassification loss**\nотступ положителен, когда знак таргета и предсказания совпадают, отрицателен, когда они не совпадают\nиз этого получается кусочно-постоянная функция, которую невозможно оптимизировать (производная равна нулю во всех точках, где она существует)\n\n### ошибка перцептрона\nидея: считаем отступы только на неправильно классифицированных объектах и учитываем их линейно, пропорционально их размеру\nиз этого получаем функцию $F(M) =\\max(0, -M)$\nс $L^2$ регуляризацией\n$$L(w,x,y) = \\lambda ||w||^2_2 + \\sum_i \\max(0, -y_i\\langle w, x_i \\rangle)$$\nградиент этой функции:\n![[Screenshot 2025-08-14 at 14.36.53.png]]решает задачу линейной классификации, но ее решение не единственно и сильно зависит от начальных параметров\n\n### Hinge loss, SVM\nхочется не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ\n\nкак это можно сделать?\n![[Screenshot 2025-08-14 at 14.39.46.png]]\n\nдобавленная единичка влияет так: те объекты, которые проклассифицированы правильно, но не очень уверено, продолжают вносить свой вклад в градиент и стремяться отодвинуть от себя разделяющую плоскость как можно дальше\n\nитоговое положени плоскости задается несколькими ближайшими к ней правильно классифицированными объектами. они являются **опорными векторами / support vectors**. этот метод называется **методом опорных векторов / support vectors machine, SVM **\n\nплюсы метода:\n- существует уникальное решение\n- доказуемо минимальная склонность к переобучению среди всех линейных классификаторов\n\n ### логистическая регрессия\n #логистическая_регрессия #сигмоида\n метод: смотрим на классификацию как на задачу предсказания вероятностей\n для этого мы учим линейную модель предсказывать **логиты (log odds**, логарифм отношение вероятности положительного событи к отрицательному) в диапазоне $(-\\infty , \\infty)$\n логиты = $log (\\frac{p}{1-p})$\n из этого вероятность будет считаться как $p = \\frac{1}{1 + e^{-\\langle w, x_i \\rangle}}$\n это - **сигмоида**\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\n\nкак нам оптимизировать w? метод максимум правдоподобия для распределения Бернулли, которое позволяет понять, насколько вероятно получить данные значения таргета y при данных Х и весах w\n$$p(y|X,w) = \\prod_i p_i^{y_i} (1 - p_i)^{1-y_i} $$\nгде $p_i$ - вероятность, посчитанная из ответов модели \nоптимизация произведения неудобна -> переходим к логарифмическому правдоподобию, получаем следующую функцию потерь\n $$L(w,X,y) = -\\sum_i(y_i\\log(\\sigma(\\langle w, x_i \\rangle)) + (1 - y_i)\\log(\\sigma(-\\langle w, x_i \\rangle)))$$\n тогда градиент\n $$\\nabla_w L(y,X,w) = -\\sum_i x_i (y_i - \\sigma(\\langle w, x_i \\rangle))$$\n предсказание модели - $p = \\sigma(\\langle w, x_i\\rangle)$\n как перейти от вероятности к классу? подобрать порог для уже построенной регрессия, минимизируя нужную нам метрику на тесте\n \n ### многоклассовая классификация\n #многоклассовая_классификация \n чтобы предсказать K классов, мы можем свести задачу многоклассовой классификации к набору бинарных классификаций, есть два самых популярных способа это сделать:\n 1. one VS all\n К линейных классификаторов $b_1(x).... b_K(x)$, выдающих оценки принадлежности классам от 1 до К\n для линейных классификаторов:\n $b_k(x) = sgn(\\langle w_k, x \\rangle + w_0k)$\n каждый из классификаторов учится отличать k-ый класс от всех остальных\n итоговый классификатор выдает класс, соответствующий самому уверенному из алгоритмов, например:\n $a(x) = argmax_k (b_k)$\n\nпроблема: каждый из классификаторов обучается на своей выборке, и значения линейных функций (= выходы классификаторов) могут иметь разные масштабы, а нормировать вектора весов для выдачи ответов в одной и той же шкале не всегда поможет\n 2. all VS all\nОбучим $C^2_K$ классификаторов $a_{ij}(x), i, j = 1,..., K, i \\neq j$\nклассификатор настраивается по подвыборке, содержащей только объекты двух классов\nкаждый из классификаторов голосует за свой класс, и мы выбираем тот класс, за который наберется больше всего голосов \n$a(x) = argmax_k \\sum^K_{i=1} \\sum_{j\\neq i}I[a_{ij}(x) = k]$\n\n### многоклассовая логистическая регрессия\n#softmax\nв логистической регрессии для двух классов мы строили линейную модель, а затем переводили ее прогноз в вероятность. \nдопустим, мы построили К моделей, каждая из которых дает оценку принадлежности объекта к одному из классов. \nдля получения вероятностей мы преобразуем вектор оценок $b_1(x),...,b_K(x)$ в вероятности с помощью оператора $softmax(z_1,...,z_K)$, который производит нормировку вектора\n$$softmax(z_1,...,z_K) = (\\frac{exp(z_1)}{\\sum^K_{k=1}exp(z_k)},..., \\frac{exp(z_K)}{\\sum^K_{k=1}exp(z_k)})$$\n\nтогда вероятность k-го класса будет выражаться как \n$$P(y=k|x,w) = \\frac{exp(\\langle w_k,x\\rangle + w_{0k})}{\\sum^K_{j=1}exp(\\langle w_j,x \\rangle + w_{0j})}$$\nобучаем эти веса с помощью метода максимального правдоподобия\n$$\\sum^N_{i=1} \\log P (y=y_i|x_i,w) \\rightarrow \\max_{w_1,..,w_K}$$\n\n### масштабируемость линейных моделей\nчто нам делать, если признаков очень много или мы не знаем заранее, сколько их будет?\n- полная размерность объекта в выборке большая, но количество ненулевых элементов в нем невелико => используем разреженное кодирование, то есть вместо плотного вектора храним словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора\n- мы можем не хранить все веса! их можно хранить в хэш-таблице и вычислять индекс по формуле `hash(feature) % tablesize%`. таким образом несколько фичей будут иметь общий вес, который обучится оптимальным образом - **hashing trick** \n\nсм. vowpal wabbit\n\n### parameter server\nЕсли при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице\n\n![[Screenshot 2025-08-14 at 16.02.32.png]]\n![1](https://yastatic.net/s3/education-portal/media/1_23_2a619a4168_e576bac717.webp)\n\nКружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много.\n\n", "created": "2025-08-14T16:02:42", "modified": "2025-08-14T16:02:42"}
{"id": "собесы/14. трансформеры.md", "title": "rnn", "tags": ["rnn"], "links": ["Screenshot 2025-08-27 at 16.09.51.png", "Screenshot 2025-08-27 at 16.10.29.png", "Screenshot 2025-08-27 at 16.49.44.png", "Screenshot 2025-08-27 at 16.50.15.png", "Screenshot 2025-08-27 at 16.51.33.png", "Screenshot 2025-08-27 at 16.52.41.png", "Screenshot 2025-08-27 at 16.54.12.png", "Screenshot 2025-08-27 at 16.55.36.png", "Screenshot 2025-08-27 at 16.59.49.png", "Screenshot 2025-08-27 at 17.02.58.png", "Screenshot 2025-08-27 at 17.03.31.png", "Screenshot 2025-08-27 at 17.03.59.png"], "content": "#rnn\nминусы rnn:\n- rnn содержит всю информацию о последовательности, которое обновляется с каждым шагом. если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации.\n- сложно распараллелить: чтобы получить скрытое состояние rnn-слоя для шага i + 1, необходимо вычислить скрытое состояние для шага i. таким образом, обработка батча примеров длиной 1000 должна потребовать 1000 последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений.\n=> тяжело применять rnn к по-настоящему длинным последовательностям\n![[Screenshot 2025-08-27 at 16.09.51.png]]\n\nэнкодер состоит из N блоков вида:\n![[Screenshot 2025-08-27 at 16.10.29.png]]\n\nдва важных слоя:\n- multi-head attention\n- feed-forward\nпосле каждого из них к выходу прибавляется вход - residual connection\nзатем активации проходят через слой layer normalization (Add & Norm)\n\nу блоков декодера похожее устройство, но внутри каждого из N блоков два слоя multi-head attention\n\n### слой внимания \nпервая часть transformer-блока — это слой self-attention. от обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым.\n\nдля этого используются три обучаемые матрицы $W_Q, W_K, W_V$, на которые умножается представление $x_i$ каждого элемента входной последовательности. получаем вектор-строки $q_i,k_i,v_i$, которые соответственно называются запросами, ключами и значениями \n\nблизость запроса к ключу можно определять например с помощью скалярного произведения $$selfattention \\ weights_i = softmax(\\frac{q_ik^T_1}{C},\\frac{q_ik^T_2}{C},...)$$где C - некоторая нормировочная константа (корень $\\sqrt{d_k}$) из размерности ключей и значений\nдалее значения $v_i$ складываются с полученными коэффициентами близости запросов к ключам: $$selfattention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nгде $Q,K,V$ - матрица зпросов, ключей и значений, в которых по строкам записаны $q_i, k_i, v_i$, а softmax берется построчно\n\n### внимание в декодере\nв декодере один из attention-слоев является слоем кросс-внимания, в котором запросы берутся из выходной последовательности, а ключи и значения - из входной\n![[Screenshot 2025-08-27 at 16.49.44.png]]\nТакже стоит учитывать, что в описанном выше виде внимания каждый токен будет «смотреть» на всю последовательность, что нежелательно для декодера. Действительно, на этапе генерации мы будем порождать по одному токену за шаг, и доступ к последующим шагам на этапе обучения приведёт к утечке информации в декодере и низкому качеству модели. Чтобы избежать этой проблемы, при обучении к вниманию нужно применять авторегрессивную маску, вручную обращая в −∞ веса до softmax для токенов из будущего, чтобы после softmax их вероятности стали нулевыми. Как можно увидеть на рисунке внизу, эта маска имеет нижнетреугольный вид.\n![[Screenshot 2025-08-27 at 16.50.15.png]]\n\n  ### Multi-head attention\n  один набор Q, K и V может отражать только один вид зависимостей между токенами, и матрицы извлекают лишь ограниченный набор информации из входных представлений. Чтобы скомпенсировать эту неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты.\n\n![[Screenshot 2025-08-27 at 16.51.33.png]]\n\n### эффективность\nподход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. в частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся.\n\nоднако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с каждым, легко показать, что сложность self-attention составляет $O(n^2)$ по длине последовательности, а простые реализации, формирующие полную матрицу внимания, будут расходовать ещё и $O(n^2)$ памяти. c оптимизацией вычислительной сложности внимания связано множество работ как инженерного, так и архитектурного плана: в частности, есть подходы, которые позволяют сократить время работы self-attention до линейного или существенно уменьшают константы за счёт учёта иерархии памяти GPU.\n\nнапример, на графиках ниже сравнивается время работы и потребление памяти трансформера со стандартным вниманием и с механизмом из [статьи](https://arxiv.org/abs/2004.05150) Longformer:\n\n![[Screenshot 2025-08-27 at 16.52.41.png]]\n\n### полносвязный слой и нормализация\nвторая часть трансформерного блока называется feed-forward network (FFN): два полносвязных слоях, применяемых независимо к каждому элементу входной последовательности\nв последних архитектурах размер промежуточного представления (то есть выхода первого слоя) бывает весьма большим — в 4 раза больше выходов блока.\nиз-за этого вычислительной стоимостью FFN не стоит пренебрегать: несмотря на квадратичную асимптотику внимания, в больших моделях или на коротких последовательностях FFN может занимать существенно больше времени по сравнению с self-attention. В виде формулы применение FFN можно представить так:\n$FFN(x)=act(xW_1+b_1)W_2+b_2$\nПромежуточные активации act в FFN бывают разными: начиналось всё с широко известной ReLU, но в какой-то момент сообщество перешло на [GELU (Gaussian Error Linear Unit)](https://arxiv.org/abs/1606.08415v4) с формулой $xΦ(x)$, где $Φ$ — функция распределения стандартной нормальной случайной величины.![[Screenshot 2025-08-27 at 16.54.12.png]]\nСкажем ещё пару слов о layer normalization: как было показано в [ряде](https://arxiv.org/abs/2002.04745) [работ](https://aclanthology.org/2020.emnlp-main.463/), их положение внутри residual-ветки довольно важно. В стандартной архитектуре используется формулировка PostLN, где нормализация применяется после остаточной связи. Однако такое применение нормализации оказывается довольно нестабильным при обучении моделей с большим числом слоёв: вместо этого предлагается использовать PreLN (справа на рисунке снизу), где нормализация применяется ко входу residual-ветки.\n![[Screenshot 2025-08-27 at 16.55.36.png]]\n\n### Кодирование \nвнимательный читатель может заметить, что все операции внутри трансформер-блока, строго говоря, инвариантны к порядку элементов в последовательности. например, результат внимания зависит от скалярных произведений между эмбеддингами токенов, но расположение этих токенов внутри текста значения не имеет. таким образом, итоговые представления каждого токена на выходе из модели будут одинаковыми вне зависимости от порядка слов, что вряд ли нас устроит. как с этим справиться?\n\nна помощь приходит такая вещь, как позиционные эмбеддинги. это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах.\n\nисторически первым подходом были фиксированные эмбеддинги, однозначно кодирующие позицию тригонометрическими функциями (ниже pos — номер позиции, i — индекс элемента в векторе, кодирующем эту позицию, d — размерность эмбеддинга): $$PE(pos,2i) = \\sin (\\frac{pos}{10000^{2i/d}})$$\n$$PE(pos,2i+1) = \\cos (\\frac{pos}{10000^{2i/d}})$$\nc момента появления архитектуры «трансформер», однако, появилось множество других способов кодировать позиции токенов. Например, можно просто сделать позиционные эмбеддинги обучаемыми наряду с эмбеддингами токенов. иной подход — напрямую учесть тот факт, что нам важны не абсолютные позиции токенов, а расстояние между ними, и обучать [_относительные_](https://arxiv.org/abs/1803.02155) позиционные представления: подобный подход заметно улучшает качество на чувствительных к порядку слов задачах, а его более современные [модификации](https://arxiv.org/abs/2108.12409)регулярно используются в самых мощных моделях.\n![[Screenshot 2025-08-27 at 16.59.49.png]]\n\n\n### Про BERT и GPT\n\nНесомненно, трансформер-модели не были бы так интересны, если бы практически все задачи NLP сейчас не решались бы с помощью этой архитектуры. главными факторами, повлиявшими на бурный рост популярности идеи self-attention, послужили два семейства хорошо всем известных архитектур — BERT и GPT, которые в некотором роде являются энкодером и декодером трансформера, которые зажили своей жизнью.\n\nмодель **GPT** (**Generative Pretrained Transformer**) хронологически [появилась](https://openai.com/research/language-unsupervised) раньше. она представляет собой обычную языковую модель, реализованную в виде последовательности слоев декодера трансформера.\n\nв качестве задачи при обучении выступает обычное предсказание следующего токена (то есть многоклассовая классификация по словарю). важно, что в качестве маски внимания как раз выступает нижнетреугольная матрица: в противном случае возникла бы утечка в данных из-за того, что токены из «прошлого» будут видеть «будущее». полученную модель можно использовать для генерации текстов и всех задач, которые на это опираются. даже ChatGPT, обученная на специальных инструкциях, по своей сути незначительно отличается от базовой модели.\n![[Screenshot 2025-08-27 at 17.02.58.png]]\nкак понятно из названия, модель **Bidirectional Encoder Representations from Transformers**(или **BERT**) отличается от GPT двунаправленностью внимания: это значит, что при обработке входной последовательности все токены могут использовать информацию друг о друге.\nэто делает такую архитектуру более удобной для задач, где нужно сделать предсказание относительно всего входа целиком без генерации, например, при классификации предложений или поиске пар похожих документов. важно, что при этом BERT не учится генерировать тексты с нуля: одна из его задач при обучении — это masked language modeling (предсказание случайно замаскированных слов по оставшимся, изображено на рисунке ниже), а вторая — next sentence prediction (предсказание по паре текстовых фрагментов, следуют они друг за другом или нет).\n\n![[Screenshot 2025-08-27 at 17.03.31.png]]\n\nзаметим, что самое ключевое отличие в моделях BERT и GPT (а не в задачах для обучения или применениях) можно свести к использованию разных видов внимания, изображенных на рисунке снизу.\nhttps://arxiv.org/abs/1810.04805\n![[Screenshot 2025-08-27 at 17.03.59.png]]\n\n\n### Тонкости обучения\n\nк сожалению, если вы просто напишете код Transformer-нейросети и попробуете сразу обучить что-то содержательное, используя привычные для других архитектур гиперпараметры, то вас с большой вероятностью постигнет неудача. оптимизационный процесс для таких моделей зачастую требуется изменить, и недостаточное внимание к этому может повлечь за собой существенные потери в итоговом качестве или вообще привести к нестабильному обучению.\n\nпервый момент, на который стоит обратить внимание, — размер батча для обучения. практически все современные Transformer-модели обучаются на больших батчах, которые для самых больших языковых моделей могут достигать миллионов токенов. разумеется, ни одна современная GPU не может обработать столько данных за один шаг: на помощь приходят распределенное обучение и чуть более универсальный [трюк](https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html#what-is-gradient-accumulation) с аккумуляцией градиентов по микробатчам.\n\nтакже в последних статьях зачастую прибегают к [увеличению размера батча](https://openreview.net/forum?id=B1Yy1BxCZ) по ходу обучения: идея заключается в том, что на ранних этапах важнее быстрее совершить много шагов градиентного спуска, а на поздних становится важнее иметь точную оценку градиента.\n\nвторой немаловажный фактор — выбор оптимизатора и расписания для learning rate. Обучить трансформер стандартным SGD, скорее всего, не выйдет: в оригинальной статье в качестве оптимизатора использовался Adam, и де-факто он остаётся стандартом до сих пор.\n\nоднако стоит заметить, что для больших размеров батча Adam порой работает плохо: из-за этого порой приходится прибегать к алгоритмам наподобие [LAMB](https://arxiv.org/abs/1904.00962), нормализующим обновления весов для каждого слоя.\n\n### Трансформеры не для текстов\n\nРазумеется, успех этого семейства архитектур на множестве текстовых задач не мог остаться незамеченным для исследователей в других доменах. Одним из наиболее ярких примеров областей, в которой Transformer-модели нашли новое приложение, несомненно, является компьютерное зрение.\n\nК примеру, [архитектура](https://arxiv.org/abs/2010.11929) ViT (Vision Transformer) в свое время побила рекорды качества по классификации изображений, задействуя идею self-attention для картинок, разделенных на множество «лоскутных» (patches) сегментов квадратной формы.\n\nКак пишут авторы статьи, идея использовать Transformer-архитектуру в зрении пришла к ним после наблюдения за успехами таких моделей в NLP: использование такого общего подхода, как self-attention, позволяет избежать необходимости явно закладывать в архитектуру особенности задачи (это ещё называют inductive bias) при достаточном времени обучения, числе параметров и размере выборки.\n\nТакже именно на трансформерах базируется генеративная часть DALL-E — модели, положившей начало активным исследованиям последних лет в генерации изображений по тексту. Концептуально DALL-E довольно проста: её можно рассматривать как авторегрессивную «языковую модель», генерирующую изображение по одному «визуальному токену» за шаг.\n\nПрименяют трансформеры и к обучению с подкреплением: ярким примером является [работа](https://arxiv.org/abs/2106.01345) Decision Transformer, в которой предлагают использовать авторегрессивное моделирование с использованием этой архитектуры для построения агента.\n\nАвторы показали, что такой же подход, который используют для генерации текстов, можно использовать для предсказания действий в динамической среде: как показано на рисунке ниже, модель последовательно принимает стандартные тройки из закодированных состояний, текущих действий и наград и в качестве ответа на каждом шаге выдаёт следующее действие.\n", "created": "2025-08-27T17:05:43", "modified": "2025-08-27T17:05:43"}