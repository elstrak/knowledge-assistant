{"chunk_id": "вышка '24-25/нереляционные БД/1. 18 января.md#1", "note_id": "вышка '24-25/нереляционные БД/1. 18 января.md", "title": "Нереляционные БД", "section": "Introduction", "text": "реляционные БД - набор табличек, в которых есть строки, эти строки обозначают один экземпляр сущности, а столбцы - характеристики, к-ю эту сущность описывают ![[Screenshot 2025-01-18 at 11.20.12.png]] отношение = таблица атрибит или поле, в атрибуте содержится данные про одну сущность, кортеж ![[Screenshot 2025-01-18 at 11.22.20.png]]добавляют в реляционные бд что-то новое постоянно, например, в mySql добавили тип данных вектор при этом мы хотели бы хранить и обрабатывать много чего ПРОБЛЕМЫ: 1. нет оптимальных механизмов хранения (специальные типы данных есть не везде 2. нет оптимальных механизмов взаимодействия (графы в реляционных бд) 3. иногда много нулей и они занимают место + не несут ценности. решения: - спарс-матрица, к-я хранит координаты заполненных значений - убрать это поле (с нулем) вообще 4. много данных, все тормозит **масштабирование** - процесс по увеличению производительности системы. масштабировать можно не только бд, это касается всех вычислительных систем масштабирование бывает 2 видов: ![[Screenshot 2025-01-18 at 11.35.02.png]] **вертикальное масштабирование:** увеличиваем 1 машину, мощнее процессоры, больше памяти. минусы: - в какой-то момент расширять это станет дорого и трудно. - сложнее вкладываться, сервера и прошивки серверов могут устаревать. **горизонтальное масштабирование:** - увеличиваем на сам комп, а их количество разделяем всё на части или по нескольким машинам: 1. **партиционирование**- разделение таблиц по нескольким частям. например, по какому-то значению или группе значений плюсы: - параллельно выполняющиеся запросы - быстрый доступ к данным в определенном диапазоне минусы: - мы просто разделили данные на одной машине визуально вот так ![[Screenshot 2025-01-18 at 11.49.21.png]] 2. **репликация** - создание копий баз данных на разных серверах плюсы: - можно разместить удаленно - балансировка нагрузки - безопасность при сгорании одного из серверов минусы: - храним полную копию - при росте числа узлов усложняется схема передачи![[Screenshot 2025-01-18 at 11.53.31.png]] master - основная БД. 2 реплики-бэкапа 3. **шардирование/шардинг** - разделение данных по разным серверам. из минусов репликации - храним полную копию таблицы, шардинг - партиционирование рпеликации (делим и храним) плюсы: - храним меньше данных в одном месте минусы: - трудности со взаимодействием между шардами - мало какие реляционные базы данных поддерживают данныц тип ![[Screenshot 2025-01-18 at 11.57.37.png]] реляционная модель плохо подходит для масштабирования: - проблемы со взаимодействием между данными - проблемы со скоростью обработки - проблемы с хранением и потерей данных ![[Screenshot 2025-01-18 at 12.00.45.png]] | вертикальное | горизонтальное | | --------------------------------------- | ---------------------------------------------- | | расширяем 1 машину | несколько машин | | ставим что-то дополнительное и улучшаем | можно скопировать БД, можно разбить на кусочки | | | | | | | ![[Screenshot 2025-01-18 at 12.01.39.png]] итого: хотим хранить любой тип данных адекватно и при этом не иметь кучи пустых значений", "tags": [], "links": ["Screenshot 2025-01-18 at 11.20.12.png", "Screenshot 2025-01-18 at 11.22.20.png", "Screenshot 2025-01-18 at 11.35.02.png", "Screenshot 2025-01-18 at 11.49.21.png", "Screenshot 2025-01-18 at 11.53.31.png", "Screenshot 2025-01-18 at 11.57.37.png", "Screenshot 2025-01-18 at 12.00.45.png", "Screenshot 2025-01-18 at 12.01.39.png", "Screenshot 2025-01-18 at 12.03.55.png", "Screenshot 2025-01-18 at 12.12.20.png", "Screenshot 2025-01-18 at 12.22.06.png", "Screenshot 2025-01-18 at 12.53.11.png", "Screenshot 2025-01-18 at 12.54.29.png", "Screenshot 2025-01-18 at 12.57.43.png", "Screenshot 2025-01-18 at 12.57.52.png", "Screenshot 2025-01-18 at 13.03.06.png", "Screenshot 2025-01-18 at 13.05.05.png", "Screenshot 2025-01-18 at 13.05.36.png", "Screenshot 2025-01-18 at 13.09.56.png", "Screenshot 2025-01-18 at 13.10.19.png", "Screenshot 2025-01-18 at 13.12.54.png", "Screenshot 2025-01-18 at 13.18.50.png", "Screenshot 2025-01-18 at 13.24.57.png", "Screenshot 2025-01-18 at 13.25.37.png", "Screenshot 2025-01-18 at 13.26.28.png", "Screenshot 2025-01-18 at 13.28.34.png", "Screenshot 2025-01-18 at 13.29.44.png", "Screenshot 2025-01-18 at 13.30.16.png", "Screenshot 2025-01-18 at 13.31.06.png"], "position": 1}
{"chunk_id": "вышка '24-25/нереляционные БД/1. 18 января.md#2", "note_id": "вышка '24-25/нереляционные БД/1. 18 января.md", "title": "Нереляционные БД", "section": "Нереляционные БД", "text": "Нереляционные БД - другая модель хранения данных она завязана на некоторых принципах построения и управления базами, но четких критериев нет. это плюс (гибкая модель данных) и минус (очень сильно отходим от четкого представления о том, что есть что) типы: ![[Screenshot 2025-01-18 at 12.03.55.png]] нереляционное хранение внедрили ещё в 1960х, но первой NoSQL СУБД считается Strozzi NoSQL. Карло Строцци переписал СУБД RDB авторства RAND под руководством Уолтера Хоббса. Программа вышла в 1998 году RBD была системой управления реляционными базами данных, использующая комплитяор SQL. Строцци переписал программу, чтобы к БД можно было обращаться с помощью shell-скриптов** Файл с программой Строцци подписал NoSQL (ого класс) ! -------------- *shell *Shell - интерфейс операционной системы. *Shell scripting означает написание скриптов ака файлов с кодом, в котором содержатся программы. такие скрипты обычно пишутся для автоматизации работы с Unix/Linux оболочки бывают разные, языки скриптования разные, ОС разные *Bash - одна из самых популярных оболочек. Такую можно использовать для управления СУБД через командную строку, если указать bash при запуске контейнера ``sudo docker exec -it mysql-docker-container bash *Скрипты полезны (этот пример баш-скрипта к-й добавляет юзера в базу данных) ``` #!/bin/bash set -e psql -v ON_ERROR_STOP=1 --username \"$POSTGRES_USER\" --dbname \"$POSTGRES_DB\" <<-ESQL CREATE USER docker ; CREATE DATABASE docker ; GRANT ALL PRIVELEGES ON DATABASE docker TO docker ; ESQL ``` -------------- ![[Screenshot 2025-01-18 at 12.12.20.png]] NoSQL начала расширяться в 2010-е NoSQL - гибкая производительная технология, позволяющая содержать как маленькие, так и большие БД. есть минусы в виде того, что возможно вам понадобится несколько серверов Подвох: ![[Screenshot 2025-01-18 at 12.22.06.png]] связь может теряться между шардами. соответственно какие-то части данныз могут не быть полными в определенный момент. нам бы хотелось чтобы даже при потере связи между серверами/шардами, они продолжали бы работать согласно Брюеру, мы можем выбрать только 2. например, в целом все согласовано и доступно, но может теряться связь между узлами существует 3 типа систем: - CP - consistent + partition tolerant - CA - consistent + always available - AP - always available + partition tolerant теорема САР немного оправдывает системы, разработчики к-х утверждают, что нельзя одновременно иметь согласованность и доступность, и во имя доступности мы чаще всего будем жертвовать согласованностью но мы не может не поддерживать согласованность совсем. можно немножко смягчить требования к модели -> нереляционные системы отвечают свойствам BASE: - Basic availability - система в общем и целом доступна, даже если что-то сломалось или упало соединение - soft-state - данные могут быть несогласованы в какие-то моменты времени (для системы с несколткими частями это легче) - eventually consistent - данные в итоге будут согласованы, даже если не сразу и не в один момент времени (например, одна реплика будет согласована с другой, но через час) CAP - всё ещё эвристическое утверждение проектирование в нереляционных БД - сложная тема по нескольким причинам: 1. зависит от типа БД в принципе: ключ-значение, документирование, графовые, колоночные 2. нереляционные БД строятся на основе принципов САР и BASE. выбор СУБД и архитектура БД будут зависеть от того, что мы хотим получить от системы ![[Screenshot 2025-01-18 at 12.53.11.png]] ![[Screenshot 2025-01-18 at 12.54.29.png]] с инжереной точки зрения доступ и взаимодействие с БД происходит по API | SQL | NoSQL | | ---------------------------------------- | ----------------------------------------------- | | powerful declarative language construct | the DB model is not relational | | schemas and metadata | focus on distributed and horizontal scalability | | consistency assistance | weak or no schema restrictions | | referential integrity and triggers | data replication is easy | | recovery and logging | easy access is provided via an API | | multi-user operation and synchronization | consistency model is not ACID | | users, roles and security | | | indexing | | | | | NoSQL хорошо масштабируется горизонтально: 1. многие виды позволяют логически разделить данные безболезненно (например, коллекции в документоориентированной базе) 2. отказались от ACID ![[Screenshot 2025-01-18 at 12.57.43.png]] ![[Screenshot 2025-01-18 at 12.57.52.png]]", "tags": [], "links": ["Screenshot 2025-01-18 at 11.20.12.png", "Screenshot 2025-01-18 at 11.22.20.png", "Screenshot 2025-01-18 at 11.35.02.png", "Screenshot 2025-01-18 at 11.49.21.png", "Screenshot 2025-01-18 at 11.53.31.png", "Screenshot 2025-01-18 at 11.57.37.png", "Screenshot 2025-01-18 at 12.00.45.png", "Screenshot 2025-01-18 at 12.01.39.png", "Screenshot 2025-01-18 at 12.03.55.png", "Screenshot 2025-01-18 at 12.12.20.png", "Screenshot 2025-01-18 at 12.22.06.png", "Screenshot 2025-01-18 at 12.53.11.png", "Screenshot 2025-01-18 at 12.54.29.png", "Screenshot 2025-01-18 at 12.57.43.png", "Screenshot 2025-01-18 at 12.57.52.png", "Screenshot 2025-01-18 at 13.03.06.png", "Screenshot 2025-01-18 at 13.05.05.png", "Screenshot 2025-01-18 at 13.05.36.png", "Screenshot 2025-01-18 at 13.09.56.png", "Screenshot 2025-01-18 at 13.10.19.png", "Screenshot 2025-01-18 at 13.12.54.png", "Screenshot 2025-01-18 at 13.18.50.png", "Screenshot 2025-01-18 at 13.24.57.png", "Screenshot 2025-01-18 at 13.25.37.png", "Screenshot 2025-01-18 at 13.26.28.png", "Screenshot 2025-01-18 at 13.28.34.png", "Screenshot 2025-01-18 at 13.29.44.png", "Screenshot 2025-01-18 at 13.30.16.png", "Screenshot 2025-01-18 at 13.31.06.png"], "position": 2}
{"chunk_id": "вышка '24-25/нереляционные БД/1. 18 января.md#3", "note_id": "вышка '24-25/нереляционные БД/1. 18 января.md", "title": "Нереляционные БД", "section": "Виды нереляционных баз данных", "text": "1. Ключ-значение референс - словарь ``` book1: { 'id': '0', 'book': 'huy' } ``` Redis (remote dictionary service aka Redis) - нереляционная субд для хранения инфы в виде \"ключ-значение\"![[Screenshot 2025-01-18 at 13.03.06.png]] является системой управление *резидентными базами данных*, то есть размещаемыми в оперативной памяти 2. документоориентированные референс - json ``` { { 'id': '0', 'id': '1', 'book': 'huy' 'book': 'pizda' } } ``` MongoDB - опенсорсная СУБД. не требует описания схемы, самая используемая документоориентированная бд ![[Screenshot 2025-01-18 at 13.05.05.png]] ![[Screenshot 2025-01-18 at 13.05.36.png]] Schemaless - отсутствие схемы или ее гибкость. например, у нас есть 2 дока, каждый из к-х описывает книгу. у книги может не быть автора, но есть редактор. может быть неизвестен год публикации или издатель в реляционной бд на месте пропусков будут нули - и они тоже занимают место. в документоориентированной бд пустые ключи можно просто пропустить. при этом добавили возможность создавать схемы в mongoDB (может быть полезно) ![[Screenshot 2025-01-18 at 13.09.56.png]] 3. Графовые референс: графы ![[Screenshot 2025-01-18 at 13.10.19.png]] Neo4j - самая популярная графовая СУБД. в качестве языка запросов - Cypher 4. колоночные ![[Screenshot 2025-01-18 at 13.12.54.png]] в реляционных бд мы все хранили по строкам, в колоночных - по колонках колоночное хранение позволяет в огромных широких таблицах выбирать только те колонки, которые мы хотим ClickHouse - разработка Яндекс. в качестве языка запросов - обычный SQL ![[Screenshot 2025-01-18 at 13.18.50.png]]![[Screenshot 2025-01-18 at 13.24.57.png]] ![[Screenshot 2025-01-18 at 13.25.37.png]] ![[Screenshot 2025-01-18 at 13.26.28.png]] ![[Screenshot 2025-01-18 at 13.28.34.png]] ![[Screenshot 2025-01-18 at 13.29.44.png]] ![[Screenshot 2025-01-18 at 13.30.16.png]] ![[Screenshot 2025-01-18 at 13.31.06.png]]", "tags": [], "links": ["Screenshot 2025-01-18 at 11.20.12.png", "Screenshot 2025-01-18 at 11.22.20.png", "Screenshot 2025-01-18 at 11.35.02.png", "Screenshot 2025-01-18 at 11.49.21.png", "Screenshot 2025-01-18 at 11.53.31.png", "Screenshot 2025-01-18 at 11.57.37.png", "Screenshot 2025-01-18 at 12.00.45.png", "Screenshot 2025-01-18 at 12.01.39.png", "Screenshot 2025-01-18 at 12.03.55.png", "Screenshot 2025-01-18 at 12.12.20.png", "Screenshot 2025-01-18 at 12.22.06.png", "Screenshot 2025-01-18 at 12.53.11.png", "Screenshot 2025-01-18 at 12.54.29.png", "Screenshot 2025-01-18 at 12.57.43.png", "Screenshot 2025-01-18 at 12.57.52.png", "Screenshot 2025-01-18 at 13.03.06.png", "Screenshot 2025-01-18 at 13.05.05.png", "Screenshot 2025-01-18 at 13.05.36.png", "Screenshot 2025-01-18 at 13.09.56.png", "Screenshot 2025-01-18 at 13.10.19.png", "Screenshot 2025-01-18 at 13.12.54.png", "Screenshot 2025-01-18 at 13.18.50.png", "Screenshot 2025-01-18 at 13.24.57.png", "Screenshot 2025-01-18 at 13.25.37.png", "Screenshot 2025-01-18 at 13.26.28.png", "Screenshot 2025-01-18 at 13.28.34.png", "Screenshot 2025-01-18 at 13.29.44.png", "Screenshot 2025-01-18 at 13.30.16.png", "Screenshot 2025-01-18 at 13.31.06.png"], "position": 3}
{"chunk_id": "CS courses, hse/алгоритмы и структуры данных/0. основные алгоритмы и структуры данных.md#1", "note_id": "CS courses, hse/алгоритмы и структуры данных/0. основные алгоритмы и структуры данных.md", "title": "0. основные алгоритмы и структуры данных", "section": "0. основные алгоритмы и структуры данных", "text": "| алгоритм | асимптотическая сложность | краткое описание | | ------------------------------ | ------------------------- | --------------------- | | пузырьковая сортировка | $O(N^2)$ | #сортировка_пузырьком | | сортировка выбором | $O(N^2)$ | #сортировка_выбором | | сортировка вставками | $O(N^2)$ | #сортировка_вставками | | сортировка подсчетом | $O(N)$ | #сортировка_подсчетом | | сортировка слиянием | $O(N\\log N)$ | #merge_sort | | быстрая сортировка | $O(N\\log N)$ | | | цифровая сортировка | $O(N K)$ | #radix_sort | | линейный поиск | | | | бинарный поиск | | | | поиск верхней и нижней границы | | | | троичный поиск | | | | статистический массив | | | | связный список | | | | саморасширяющийся массив | | | | стек | | | | очередь | | | | дек | | | | приоритетная очередь | | | | куча | | | | сортировка кучей | | | 1. Понятие асимптотической сложности. В чем плюсы и минусы асимптотической оценки сложности 2. Виды сортировок. Квадратичные сортировки. Сортировка подсчетом. Цифровая сортировка 3. Сортировка слиянием и быстрая сортировка. 4. Сортировка кучей 5. Алгоритмы поиска. Бинарный поиск. Правая и левая границы 6. Устройство массива. Как работает статический и динамический массив, асимптотика операций. 7. Связный список. Типы связных списков. Чем отличается от массива, как устроен внутри, асимптотика. 8. Как работает стек, его асимптотика. Как реализовать стек через массив или список 9. Как работает очередь, ее асимптотика. Как реализовать очередь через массив или список 10. Как работает приоритетная очередь, ее асимптотика. Реализация приоритетной очереди. 11. Понятия хеш-функции и хеш-таблицы. Коллизия. Способы разрешения коллизий. 12. Динамическое программирование. Как решать задачи ДП. Задачи ДП про кузнечика и черепашку. 13. Задача о рюкзаке. Расстояние Левенштейна. 14. Наибольшая общая подпоследовательность. Наибольшая возрастающая подпоследовательность 15. Хеширование строк. Полиномиальный хеш. Способы применения. 16. Префикс функция. Алгоритм Кнута-Морриса-Пратта. 17. Графы. Способы хранения графа. Обход графа в глубину и в ширину. 18. Поиск компонент связности и циклов в графе. Топологическая сортировка. 19. Поиск кратчайших путей в графе. Алгоритм Дейкстры 20. Алгоритм Форда-Беллмана. Поиск цикла отрицательного веса", "tags": ["сортировка_подсчетом", "сортировка_пузырьком", "сортировка_выбором", "сортировка_вставками", "radix_sort", "merge_sort"], "links": [], "position": 1}
{"chunk_id": "собесы/2. линейные модели.md#1", "note_id": "собесы/2. линейные модели.md", "title": "Линейная регрессия и МНК", "section": "Introduction", "text": "Математически задачи можно описать как поиск отображения из множества Х в множество таргетов: - **классификация**: $X→\\{0,1,…,K\\}$, где $0,…,K$ – номера классов - **регрессия**: $X→R$. нам нужно найти такое отображение, которое лучше всего приближает истинное значение из множества таргетов. для линейных моделей мы ищем это отображение в определенном параметризированном семействе функций - в линейных функциях вида $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + w_0$$ $(x_1,x_2..., x_D)$ - вектор признаков, соответствующий объекту выборки $(w_1,w_2..., w_D, w_0)$ - вектор параметров модели, весов на предсказание можно смотреть как на взвешенную сумма признаков объекта $w_0$ - свободный коэффициент, или сдвиг (bias) линейные модели хороши тем, что просты и легко интерпретируемы", "tags": ["softmax", "регуляризация", "линейная_классификация", "многоклассовая_классификация", "линейная_регрессия", "метод_наименьших_квадратов", "сигмоида", "стохастический_гд", "MSE", "градиент", "логистическая_регрессия"], "links": ["Screenshot 2025-08-14 at 14.36.53.png", "Screenshot 2025-08-14 at 14.39.46.png", "Screenshot 2025-08-14 at 16.02.32.png"], "position": 1}
{"chunk_id": "собесы/2. линейные модели.md#2", "note_id": "собесы/2. линейные модели.md", "title": "Линейная регрессия и МНК", "section": "Линейная регрессия и МНК", "text": "#метод_наименьших_квадратов #линейная_регрессия моделируем задачу как функцию вида $f_w(x_i) = <w, x_i>$ мы хотим, чтобы наша функция как можно лучше приближала зависимость на парах $(x_i, y_i)$ = измеряем качество модели и минимизируем ее ошибку, меняя обучаемые параметры функция, оценивающая, как часто модель ошибается - **функция потерь**. она должна быть легко оптимизируема возьмем квадрат $L^2$ нормы вектора разницы предсказаний модели и истинного таргета y: $$L(f,X,y) = \\sum^N_{i=1} (y_i - <x_i,w>)^2$$ не очень хорош, для выборок разного размера. лучше смотреть на среднеквадратичное отклонение (MSE) #MSE $$L(f,X,y) = \\frac{1}{N} \\sum^N_{i=1} (y_i - <x_i,w>)^2$$ **функционалом** называют функции, которые получают на вход функции, а на выходе выдают число, оценивающее работу этой функции. чем меньше это число, тем точнее инпут функция, то есть нам надо минимизировать функционал по w: $$||y - Xw||^2_2 \\rightarrow \\min_w$$ два решения: 1) точное аналитическое задача звучит как \"найти линейную комбинацию столбцов $x^1,…,x^D$, которая наилучшим способом приближает столбец y по евклидовой норме – то есть найти **проекцию** вектора y на подпространство, образованное векторами $x^1,…,x^D$\" вычислительная сложность аналитического решения - $O(D^2N+D^3)$, где $N$ - длина выборки, $D$ - число признаков у одного объекта проблема точного решения: - вычислительное обращение больших матриц затратно 2) приближенный числовой метод #градиент Градиент функции - это вектор, который указывает направление наискорейшего возрастания функции в данной точке, а антиградиент - в сторону ее наискорейшего убывания имея некоторое приближение параметра w, мы можем его улучшить, посчитав градиент функции потерь в точке и сдвинув вектор весов в направлении антиградиента: $$w_j \\rightarrow w_j - \\alpha \\frac{d}{dw_j}L(f_w,X,y)$$ $\\alpha$ - **темп обучения**, контролирует величину шага в направлении антиградиента градиентный спуск для функции потерь MSE: $$\\nabla_wL = \\frac{2}{N}X^T(Xw - y)$$ алгоритм градиентного спуска: ``` w = random_normal() # инициализация repeat S times: f = X.dot(w) err = f - y grad = 2 * X.T.dot(err) / N w -= alpha * grad ``` вычислительная сложность градиентного спуска - $O(NDS)$, где N - длина выборки, D - число признаков у одного объекта, S - количество итераций", "tags": ["softmax", "регуляризация", "линейная_классификация", "многоклассовая_классификация", "линейная_регрессия", "метод_наименьших_квадратов", "сигмоида", "стохастический_гд", "MSE", "градиент", "логистическая_регрессия"], "links": ["Screenshot 2025-08-14 at 14.36.53.png", "Screenshot 2025-08-14 at 14.39.46.png", "Screenshot 2025-08-14 at 16.02.32.png"], "position": 2}
{"chunk_id": "собесы/2. линейные модели.md#3", "note_id": "собесы/2. линейные модели.md", "title": "Линейная регрессия и МНК", "section": "стохастический градиентный спуск", "text": "#стохастический_гд на каждом шаге ГД мы вычисляем градиент по всей выборке, что может быть потенциально дорогим, поэтому мы заменяем градиент по всей выборке оценкой на на подвыборке = на батче если функция потерь имеет вид суммы по отдельным парам объект таргет: $$ L(w,X,y) = \\frac{1}{N} \\sum^N_{i=1} L (w,x_i, y_i)$$ а градиент выглядит так: $$\\nabla_w L(w,X,y) = \\frac{1}{N} \\sum^N_{i=1}\\nabla_wL(w,x_i,y_i)$$ то мы берем оценку $$\\nabla_wL(w,X,y) \\sim \\frac{1}{B} \\sum^B_{t=1}\\nabla_wL(w,x_{i_t},y_{i_t})$$ для некотрого подмножества пар (батч) вводим новые гиперпараметры B(размер батча) и E (количество эпох) алгоритм: ``` w = normal(0,1) repeat E times: for i = B, i <=n, i+B X_batch = X[i-B : i] y_batch = y[i-B : i] f = X_batch.dot(w) # посчитать предсказание err = f - y_batch # посчитать ошибку grad = 2 * X_batch.T.dot(err) / B # посчитать градиент w -= alpha * grad ``` сложность - $O(NDE)$ сделали в $N/B$ раз больше шагов, то есть веса претерпели больше обновлений, но сложность такая же, как у обычного ГД cложность по памяти можно довести до $O(BD)$: ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). при этом лучше брать побольше размер батча, потому что чтение с диска - более затратная по времени операция, чем чтение из оперативной памяти", "tags": ["softmax", "регуляризация", "линейная_классификация", "многоклассовая_классификация", "линейная_регрессия", "метод_наименьших_квадратов", "сигмоида", "стохастический_гд", "MSE", "градиент", "логистическая_регрессия"], "links": ["Screenshot 2025-08-14 at 14.36.53.png", "Screenshot 2025-08-14 at 14.39.46.png", "Screenshot 2025-08-14 at 16.02.32.png"], "position": 3}
{"chunk_id": "собесы/2. линейные модели.md#4", "note_id": "собесы/2. линейные модели.md", "title": "Линейная регрессия и МНК", "section": "регуляризация *повторить", "text": "#регуляризация если два признака линейно зависимы, ранг матрицы будет D => будет более чем одно решение, и оно может быть сколь угодно большим по модулю => вычислительная трудность такое редко, но часто признаки могут быть приближенно линейно зависимыми - это ситуация **мультиколлинеарности** проблемы, описанные выше, будут возникать и в этой ситуации для решения этой проблемы, задачу регуляризуют - добавляют дополнительные ограничения на вектор весов это ограничение - $L^1$ или $L^2$ норма задача будет выглядеть следующим образом: $$\\min_wL(f,X,y) = \\min_w(||Xw-y||^2_2 + \\lambda||w||^k_k)$$ где $||w||^k_k$ может быть $$||w||^2_2 = w^2_1 + ... + w^2_D$$ либо $$||w||_1^1 = |w_1| + ... + |w_D| $$ эта добавка называется регуляризационным членом или регуляризатором, а лямбда - коэффициентом регуляризации коэффициент лямбла подбирают по логарифмической шкале и он сильно влияет на качество модели. вес $w_0$ соответствует отступу от начала координат (признаку из всех единичек), и он не регуляризуется градиент функции потерь после этого выглядит так: $L(f_w,X,y) = ||X_w - y||^2 + \\lambda||w||^2$ $\\nabla_wL(f_w,X,y) = 2X^T(Xw - y) + 2\\lambda w$ регуляризацию можно определеять для любой функции потерь, в том числе и для задачи классификации одна из полезных особенностей $L^1$ регуляризации - ее применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результат регуляризация обнуляется = удаляем признаки, слабо влияющие на таргет.", "tags": ["softmax", "регуляризация", "линейная_классификация", "многоклассовая_классификация", "линейная_регрессия", "метод_наименьших_квадратов", "сигмоида", "стохастический_гд", "MSE", "градиент", "логистическая_регрессия"], "links": ["Screenshot 2025-08-14 at 14.36.53.png", "Screenshot 2025-08-14 at 14.39.46.png", "Screenshot 2025-08-14 at 16.02.32.png"], "position": 4}
{"chunk_id": "собесы/14. трансформеры.md#1", "note_id": "собесы/14. трансформеры.md", "title": "rnn", "section": "Introduction", "text": "#rnn минусы rnn: - rnn содержит всю информацию о последовательности, которое обновляется с каждым шагом. если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации. - сложно распараллелить: чтобы получить скрытое состояние rnn-слоя для шага i + 1, необходимо вычислить скрытое состояние для шага i. таким образом, обработка батча примеров длиной 1000 должна потребовать 1000 последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений. => тяжело применять rnn к по-настоящему длинным последовательностям ![[Screenshot 2025-08-27 at 16.09.51.png]] энкодер состоит из N блоков вида: ![[Screenshot 2025-08-27 at 16.10.29.png]] два важных слоя: - multi-head attention - feed-forward после каждого из них к выходу прибавляется вход - residual connection затем активации проходят через слой layer normalization (Add & Norm) у блоков декодера похожее устройство, но внутри каждого из N блоков два слоя multi-head attention", "tags": ["rnn"], "links": ["Screenshot 2025-08-27 at 16.09.51.png", "Screenshot 2025-08-27 at 16.10.29.png", "Screenshot 2025-08-27 at 16.49.44.png", "Screenshot 2025-08-27 at 16.50.15.png", "Screenshot 2025-08-27 at 16.51.33.png", "Screenshot 2025-08-27 at 16.52.41.png", "Screenshot 2025-08-27 at 16.54.12.png", "Screenshot 2025-08-27 at 16.55.36.png", "Screenshot 2025-08-27 at 16.59.49.png", "Screenshot 2025-08-27 at 17.02.58.png", "Screenshot 2025-08-27 at 17.03.31.png", "Screenshot 2025-08-27 at 17.03.59.png"], "position": 1}
{"chunk_id": "собесы/14. трансформеры.md#2", "note_id": "собесы/14. трансформеры.md", "title": "rnn", "section": "слой внимания", "text": "первая часть transformer-блока — это слой self-attention. от обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым. для этого используются три обучаемые матрицы $W_Q, W_K, W_V$, на которые умножается представление $x_i$ каждого элемента входной последовательности. получаем вектор-строки $q_i,k_i,v_i$, которые соответственно называются запросами, ключами и значениями близость запроса к ключу можно определять например с помощью скалярного произведения $$selfattention \\ weights_i = softmax(\\frac{q_ik^T_1}{C},\\frac{q_ik^T_2}{C},...)$$где C - некоторая нормировочная константа (корень $\\sqrt{d_k}$) из размерности ключей и значений далее значения $v_i$ складываются с полученными коэффициентами близости запросов к ключам: $$selfattention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$ где $Q,K,V$ - матрица зпросов, ключей и значений, в которых по строкам записаны $q_i, k_i, v_i$, а softmax берется построчно", "tags": ["rnn"], "links": ["Screenshot 2025-08-27 at 16.09.51.png", "Screenshot 2025-08-27 at 16.10.29.png", "Screenshot 2025-08-27 at 16.49.44.png", "Screenshot 2025-08-27 at 16.50.15.png", "Screenshot 2025-08-27 at 16.51.33.png", "Screenshot 2025-08-27 at 16.52.41.png", "Screenshot 2025-08-27 at 16.54.12.png", "Screenshot 2025-08-27 at 16.55.36.png", "Screenshot 2025-08-27 at 16.59.49.png", "Screenshot 2025-08-27 at 17.02.58.png", "Screenshot 2025-08-27 at 17.03.31.png", "Screenshot 2025-08-27 at 17.03.59.png"], "position": 2}
{"chunk_id": "собесы/14. трансформеры.md#3", "note_id": "собесы/14. трансформеры.md", "title": "rnn", "section": "внимание в декодере", "text": "в декодере один из attention-слоев является слоем кросс-внимания, в котором запросы берутся из выходной последовательности, а ключи и значения - из входной ![[Screenshot 2025-08-27 at 16.49.44.png]] Также стоит учитывать, что в описанном выше виде внимания каждый токен будет «смотреть» на всю последовательность, что нежелательно для декодера. Действительно, на этапе генерации мы будем порождать по одному токену за шаг, и доступ к последующим шагам на этапе обучения приведёт к утечке информации в декодере и низкому качеству модели. Чтобы избежать этой проблемы, при обучении к вниманию нужно применять авторегрессивную маску, вручную обращая в −∞ веса до softmax для токенов из будущего, чтобы после softmax их вероятности стали нулевыми. Как можно увидеть на рисунке внизу, эта маска имеет нижнетреугольный вид. ![[Screenshot 2025-08-27 at 16.50.15.png]] ### Multi-head attention один набор Q, K и V может отражать только один вид зависимостей между токенами, и матрицы извлекают лишь ограниченный набор информации из входных представлений. Чтобы скомпенсировать эту неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты. ![[Screenshot 2025-08-27 at 16.51.33.png]]", "tags": ["rnn"], "links": ["Screenshot 2025-08-27 at 16.09.51.png", "Screenshot 2025-08-27 at 16.10.29.png", "Screenshot 2025-08-27 at 16.49.44.png", "Screenshot 2025-08-27 at 16.50.15.png", "Screenshot 2025-08-27 at 16.51.33.png", "Screenshot 2025-08-27 at 16.52.41.png", "Screenshot 2025-08-27 at 16.54.12.png", "Screenshot 2025-08-27 at 16.55.36.png", "Screenshot 2025-08-27 at 16.59.49.png", "Screenshot 2025-08-27 at 17.02.58.png", "Screenshot 2025-08-27 at 17.03.31.png", "Screenshot 2025-08-27 at 17.03.59.png"], "position": 3}
{"chunk_id": "собесы/14. трансформеры.md#4", "note_id": "собесы/14. трансформеры.md", "title": "rnn", "section": "эффективность", "text": "подход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. в частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся. однако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с каждым, легко показать, что сложность self-attention составляет $O(n^2)$ по длине последовательности, а простые реализации, формирующие полную матрицу внимания, будут расходовать ещё и $O(n^2)$ памяти. c оптимизацией вычислительной сложности внимания связано множество работ как инженерного, так и архитектурного плана: в частности, есть подходы, которые позволяют сократить время работы self-attention до линейного или существенно уменьшают константы за счёт учёта иерархии памяти GPU. например, на графиках ниже сравнивается время работы и потребление памяти трансформера со стандартным вниманием и с механизмом из [статьи](https://arxiv.org/abs/2004.05150) Longformer: ![[Screenshot 2025-08-27 at 16.52.41.png]]", "tags": ["rnn"], "links": ["Screenshot 2025-08-27 at 16.09.51.png", "Screenshot 2025-08-27 at 16.10.29.png", "Screenshot 2025-08-27 at 16.49.44.png", "Screenshot 2025-08-27 at 16.50.15.png", "Screenshot 2025-08-27 at 16.51.33.png", "Screenshot 2025-08-27 at 16.52.41.png", "Screenshot 2025-08-27 at 16.54.12.png", "Screenshot 2025-08-27 at 16.55.36.png", "Screenshot 2025-08-27 at 16.59.49.png", "Screenshot 2025-08-27 at 17.02.58.png", "Screenshot 2025-08-27 at 17.03.31.png", "Screenshot 2025-08-27 at 17.03.59.png"], "position": 4}